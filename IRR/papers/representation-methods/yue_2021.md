**Model**
- Fully connected layer
- CNN blocks: Dilated Conv + GELU
- 10 blocks deep
- Heirarchical contrastive loss, augmenting two copies of input $x$ with masking and cropping, and then treating corresponding timestamps in these two augmentations as positive pairs. Negative samples are drawn from other timestamps and other instances (samples in X). This contrastive loss is also computed at multiple levels of depth in the network, such that latent representations of corresponding timesteps are encouraged to be nearby in feature space.
- This loss differs from [[franceschi_2020]] in that it takes negative samples from _representations_ of other instances as well as from raw instances.
- They compare representations multiple levels of abstraction, using max-pooling to create representations at longer time scales.

(better explanation)
- Heirarchical constrastive loss. Two copies of each training instance augmented via masking and cropping. Corresponding timestamps then form positive samples, with negative samples being the non-corresponding timestamps from this instance (temporal contrastive loss), as well those of other instances (instance-wise contrastive loss). Then, max-pooling is repeatedly applied to the representations, forming representations that span more and more of the instance, and temporal and instance-wise contrastive loss are computed at each level. The loss is thus derived from a heirarchy of representations. This differs from [[franceschi_2020]], in which negative samples are drawn only from other raw instances, and not at other timestamps or at multiple levels of abstraction. [[franceschi_2020]] further differs in that positive samples have a series-subseries relationship, rather than corresponding timesteps of two augmented version of the training instance.

**Transfer**
- Also trains encoder on FordA, as with [[franceschi_2020]], and then fits SVM to UCR datasets, and finds better performance than [[franceschi_2020]].